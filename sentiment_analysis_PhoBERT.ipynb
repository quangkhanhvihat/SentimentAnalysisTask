{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment_analysis_PhoBERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRBM76cT_9uK",
        "outputId": "2c3a7285-9ec7-430b-957e-782e753ebfd1"
      },
      "source": [
        "!git clone https://github.com/ntienhuy/MultiChannel\n",
        "# !npx degit ntienhuy/MultiChannel/tree/master/data/VS/Data_not_token/Fold_1 -f"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MultiChannel'...\n",
            "remote: Enumerating objects: 135, done.\u001b[K\n",
            "remote: Total 135 (delta 0), reused 0 (delta 0), pack-reused 135\u001b[K\n",
            "Receiving objects: 100% (135/135), 8.96 MiB | 4.46 MiB/s, done.\n",
            "Resolving deltas: 100% (52/52), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T21X__cmJAty",
        "outputId": "287a3046-f8c2-47b8-d292-5c8dcab9fb56"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UDQZc01QJBV"
      },
      "source": [
        "import regex as re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76S_yjMY0Wkr"
      },
      "source": [
        "MAX_LEN = 50\n",
        "BATCH_SIZE = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6wLCfTSPVUf"
      },
      "source": [
        "# Ham xu ly chuỗi sai dấu\n",
        "# ví dụ:  'Nhe ̣ nha ̀ ng sâu lă ́ ng nhưng cu ̃ ng triê ́ t ly ́ va ̀ tinh tê ́ .'\n",
        "def chuan_hoa_dau(s):\n",
        "    dau_nang_sai = ' ̣ '\n",
        "    dau_nang_dung = dau_nang_sai[1]\n",
        "    dau_huyen_sai = ' ̀ '\n",
        "    dau_huyen_dung = dau_huyen_sai[1]\n",
        "    dau_sac_sai = ' ́ '\n",
        "    dau_sac_dung = dau_sac_sai[1]\n",
        "    dau_nga_sai = ' ̃ '\n",
        "    dau_nga_dung = dau_nga_sai[1]\n",
        "    dau_hoi_sai = ' ̉ '\n",
        "    dau_hoi_dung = dau_hoi_sai[1]\n",
        "    dau_cau = {dau_nang_sai : dau_nang_dung,\n",
        "           dau_huyen_sai : dau_huyen_dung,\n",
        "           dau_nga_sai : dau_nga_dung,\n",
        "           dau_sac_sai : dau_sac_dung,\n",
        "           dau_hoi_sai : dau_hoi_dung}\n",
        "    nguyen_am = 'aáàạảãăắằằẳẵâấầậẩẫeéèẻèẽêếềểễệiỉíìĩịoóòỏõọôốồộổỗơớờởỡợuúùụủũưứừựửữyýỳỷỹỵ'\n",
        "\n",
        "    for k, v in dau_cau.items():\n",
        "        while(s.find(k) >= 0):\n",
        "            pos = s.find(k)\n",
        "            s1 = s[pos+3:]\n",
        "            # s2 là những ký tự phía sau dấu câu sai\n",
        "            s2 = s1[:s1.find(' ')]\n",
        "            if len(s2) == 2:\n",
        "                #phía sau là môt từ\n",
        "                if (nguyen_am.find(s2[0])>=0) | (nguyen_am.find(s2[1])>=0):\n",
        "                    s=s.replace(k,v + ' ',1)\n",
        "                #phia sau k phai 1 tu\n",
        "                else: \n",
        "                    s=s.replace(k,v,1)\n",
        "            #phia sau la mot tu\n",
        "            elif len(s2) > 2:\n",
        "                s=s.replace(k,v + ' ',1)\n",
        "            #phia sau chi co 1 ky tu va no la 1 tu dac biet ngan\n",
        "            elif 'áàạéèẻèêếềiíịoóòỏôốồơớờởợuúùụủũưứừựyýỳỵ'.find(s)>0:\n",
        "                s=s.replace(k,v + ' ',1)\n",
        "            else:\n",
        "                s=s.replace(k,v,1)\n",
        "    return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebwA6QscPWK2",
        "outputId": "0d7a31e8-3a2e-4f3d-da20-89a4e010adfc"
      },
      "source": [
        "!gdown --id 1TzXkIicD3MsI6ciKnsDfSX4vFmYqd3LV\n",
        "#download teencode.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1TzXkIicD3MsI6ciKnsDfSX4vFmYqd3LV\n",
            "To: /content/teencode.txt\n",
            "\r  0% 0.00/5.66k [00:00<?, ?B/s]\r100% 5.66k/5.66k [00:00<00:00, 4.66MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z8TwG52Pay0"
      },
      "source": [
        "f = open('/content/teencode.txt','r', encoding='UTF-8')\n",
        "teencode_dict = {}\n",
        "for aline in f.readlines():\n",
        "    aline = aline.split()\n",
        "    teencode_dict[aline[0]] = ' '.join(aline[1:])\n",
        "# teencode_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSZpWI3_PdRs"
      },
      "source": [
        "def load_data(folder_id):\n",
        "    train_data = []\n",
        "    train_labels = []    \n",
        "    test_data = []\n",
        "    test_labels = []\n",
        "    for label in range(3):\n",
        "\n",
        "        # read train data\n",
        "        path = '/content/MultiChannel/data/VS/Data_Token/Fold_{}/train_nhan_{}.txt'.format(str(folder_id),str(label))\n",
        "        f = open(path, \"r\",encoding='utf-8-sig')\n",
        "        regex = re.compile('[-:()\\\",.!?=`~/+&$%^*#]')\n",
        "        text = []\n",
        "        for aline in f.readlines():\n",
        "            aline = chuan_hoa_dau(aline)\n",
        "            aline = aline.lower()\n",
        "            aline = regex.sub('', aline).strip()\n",
        "            aline = re.sub(' +',' ',aline)\n",
        "            for k,v in teencode_dict.items():\n",
        "                aline.replace(k,v)\n",
        "            if len(aline.split())==0:\n",
        "                continue\n",
        "            text.append(aline)\n",
        "        train_labels += [label]*len(text)\n",
        "        print('label: {}, len: {}'.format(label, len(text)))\n",
        "        train_data +=text\n",
        "        f.close()\n",
        "\n",
        "        # read test data\n",
        "        path = '/content/MultiChannel/data/VS/Data_Token/Fold_{}/test_nhan_{}.txt'.format(str(folder_id),str(label))\n",
        "        f = open(path, \"r\",encoding='utf-8-sig')\n",
        "        regex = re.compile('[-:()\\\",.!?=`~/+&$%^*#]')\n",
        "        text = []\n",
        "        for aline in f.readlines():\n",
        "            aline = chuan_hoa_dau(aline)\n",
        "            aline = aline.lower()\n",
        "            aline = regex.sub('', aline).strip()\n",
        "            aline = re.sub(' +',' ',aline)\n",
        "            for k,v in teencode_dict.items():\n",
        "                aline.replace(k,v)\n",
        "            if len(aline.split())==0:\n",
        "                continue\n",
        "            text.append(aline)\n",
        "        test_labels += [label]*len(text)\n",
        "        test_data += text\n",
        "        f.close()\n",
        "    return train_data, train_labels, test_data, test_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gY4WsSePHfvU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72e5208c-3d4f-455e-dbe3-02a82be82d93"
      },
      "source": [
        "folder_id = 1\n",
        "train_data, train_labels, test_data, test_labels = load_data(folder_id=folder_id)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label: 0, len: 9446\n",
            "label: 1, len: 7304\n",
            "label: 2, len: 9292\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpVP4U7C1e9C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e8613b4-81bf-4a55-e131-40b9dfca7421"
      },
      "source": [
        "print(len(train_data))\n",
        "print(len(train_labels))\n",
        "print(len(test_data))\n",
        "print(len(test_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26042\n",
            "26042\n",
            "8917\n",
            "8917\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXk-76XuuVBf"
      },
      "source": [
        "!pip install transformers\n",
        "import transformers\n",
        "from transformers import AutoModel, AutoTokenizer, BertModel\n",
        "import torch\n",
        "phobert_model = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s12eIO0VwSQN"
      },
      "source": [
        "def preprocessing_for_phobert(data):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    # token_type_ids = []\n",
        "    for sentence in data:\n",
        "        encoded = tokenizer.encode_plus(\n",
        "            text = sentence,\n",
        "            add_special_tokens = True,\n",
        "            max_length = MAX_LEN,\n",
        "            truncation  = True,\n",
        "            pad_to_max_length = True,\n",
        "            return_token_type_ids = True,\n",
        "            return_attention_mask = True,\n",
        "            # return_tensors = 'pt'\n",
        "        )\n",
        "        input_ids.append(encoded.get('input_ids'))\n",
        "        attention_masks.append(encoded.get('attention_mask'))\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_masks = torch.tensor(attention_masks)\n",
        "\n",
        "    return input_ids, attention_masks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtJ-zgIP17AR",
        "outputId": "de6e8a88-00c4-4acd-8ff3-6d1145317270"
      },
      "source": [
        "train_inputs, train_masks = preprocessing_for_phobert(train_data)\n",
        "test_inputs, test_masks = preprocessing_for_phobert(test_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAh6GHNZ0MrU"
      },
      "source": [
        "train_labels = torch.tensor(train_labels)\n",
        "train_dataset = torch.utils.data.TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                                           pin_memory =True, shuffle=True)\n",
        "test_labels = torch.tensor(test_labels)\n",
        "test_dataset = torch.utils.data.TensorDataset(test_inputs, test_masks, test_labels)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                                           pin_memory =True, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PO_r-XESss_"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "# from sklearn.SVM import SVC\n",
        "class MyClassifier(nn.Module):\n",
        "    def __init__(self, num_labels , freeze_phobert = False):\n",
        "        super(MyClassifier, self).__init__()\n",
        "        self.num_labels = num_labels\n",
        "        # self.phobert = phobert_model\n",
        "        self.fc_1 = nn.Linear(768, 64)\n",
        "        self.fc_2 = nn.Linear(64,num_labels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # if freeze_phobert:\n",
        "        #     for param in self.phobert.parameters():\n",
        "        #         param.requires_grad = False\n",
        "        ###\n",
        "    def forward(self,x):\n",
        "        # phobert_outputs = self.phobert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        # cls = phobert_outputs[0][:, 0, :].squeeze()\n",
        "        # print(cls.shape)\n",
        "        x = self.relu(self.fc_1(x))\n",
        "        x = self.fc_2(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxO1m_IRZlSp"
      },
      "source": [
        "\n",
        "def phobert_training_loop(n_epochs, optimizer, model, loss_fn, train_loader, test_loader):\n",
        "    highest_acc = 0.6361\n",
        "    model.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/sentiment_analysis/folder{}_PhoBert.pth\".format(folder_id)))\n",
        "    for epoch in range(1,n_epochs+1):\n",
        "        model.train()\n",
        "        loss_train = 0.0\n",
        "        for input_ids,attn_mask ,labels in train_loader:\n",
        "\n",
        "            input_ids = input_ids.type(torch.LongTensor)\n",
        "            attn_mask = attn_mask.type(torch.LongTensor)\n",
        "            labels = labels.type(torch.LongTensor)\n",
        "\n",
        "            phobert_outputs = phobert_model(input_ids=input_ids.to(device), attention_mask=attn_mask.to(device))\n",
        "            cls = phobert_outputs[0][:, 0, :].squeeze()\n",
        "            outputs = model(cls)\n",
        "            # print(outputs[:5])\n",
        "            optimizer.zero_grad()\n",
        "            loss = loss_fn(outputs, labels.to(device))\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_train +=loss.item()\n",
        "            # break\n",
        "        correct = 0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for data in test_loader:\n",
        "                input_id ,att_mask ,label = data\n",
        "                input_id = input_id.type(torch.LongTensor)\n",
        "                att_mask = att_mask.type(torch.LongTensor)\n",
        "                label = label.type(torch.LongTensor)\n",
        "\n",
        "                phobert_output = phobert_model(input_ids=input_id.to(device), attention_mask=att_mask.to(device))\n",
        "                cls = phobert_output[0][:, 0, :].squeeze()\n",
        "                output = model(cls)\n",
        "                predicted = torch.argmax(output,1)\n",
        "                # print(predicted[:5])\n",
        "                c = (predicted == label.to(device))#.squeeze()\n",
        "                correct += c.sum()\n",
        "                # break\n",
        "        if epoch == 1 or epoch % 1 == 0:\n",
        "            print('Epoch {}, Training loss {}, Val accuracy {}'.format(\n",
        "                epoch,\n",
        "                loss_train / len(train_loader),\n",
        "                correct / len(test_labels)))\n",
        "        if correct/len(test_labels) > highest_acc:\n",
        "            highest_acc = correct/len(test_labels)\n",
        "            print(highest_acc)\n",
        "            torch.save(model.state_dict(), \"/content/drive/MyDrive/Colab Notebooks/sentiment_analysis/folder{}_PhoBert.pth\".format(folder_id))\n",
        "            # break\n",
        "        # break\n",
        "    print(highest_acc)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "my7cTO3qbQQC",
        "outputId": "4bc423da-8a48-4333-93db-a14e35ec176c"
      },
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Picking GPU if available or else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "device = get_default_device()\n",
        "\n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "class DevicedataLoader():\n",
        "    def __init__(self,dl,device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "    def __iter__(self):\n",
        "        for b in self.dl:\n",
        "            yield to_device(b, self.device)\n",
        "    def __len__(self):\n",
        "        return len(self.dl)\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sv68JYyWBZgN"
      },
      "source": [
        "train_loader = DevicedataLoader(train_loader,device)\n",
        "test_loader = DevicedataLoader(test_loader,device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zwUwVW6ZlWa"
      },
      "source": [
        "num_labels = 3\n",
        "learning_rate = 1e-4\n",
        "phobert_model.to(device)\n",
        "phobert = MyClassifier(num_labels = num_labels,freeze_phobert=True)\n",
        "phobert.to(device)\n",
        "optimizer = optim.Adam(phobert.parameters(), lr=learning_rate)\n",
        "loss_fn = F.cross_entropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyEMrzJkdhi6"
      },
      "source": [
        "phobert_training_loop(\n",
        "    n_epochs = 10,\n",
        "    optimizer = optimizer,\n",
        "    model = phobert,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        "    test_loader = test_loader\n",
        ")"
      ],
      "execution_count": 2,
      "outputs": []
    }
  ]
}