{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af164f1b-2aa4-4c96-8c26-46029502b490",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "# Pos Padding _CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-sierra",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'MultiChannel' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ntienhuy/MultiChannel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-arthritis",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "def load_data(folder_id):\n",
    "    train_data = []\n",
    "    train_labels = []    \n",
    "    test_data = []\n",
    "    test_labels = []\n",
    "    for label in range(3):\n",
    "        \n",
    "        # read train data\n",
    "        path = 'MultiChannel/data/VS/Data_Token/Fold_{}/train_nhan_{}.txt'.format(str(folder_id),str(label))\n",
    "        f = open(path, \"r\",encoding='utf-8-sig')\n",
    "        regex = re.compile('[-:()\\\",.!?=`~/+&$%^*#]')\n",
    "        text = []\n",
    "        for aline in f.readlines():\n",
    "            aline = aline.lower()\n",
    "            aline = regex.sub('', aline).strip()\n",
    "            aline = re.sub(' +',' ',aline)\n",
    "            text.append(aline)\n",
    "        train_labels += [label]*len(text)\n",
    "        train_data +=text\n",
    "        print('label: {}, len: {}'.format(label, len(text)))\n",
    "\n",
    "        # read test data\n",
    "        path = 'MultiChannel/data/VS/Data_Token/Fold_{}/test_nhan_{}.txt'.format(str(folder_id),str(label))\n",
    "        f = open(path, \"r\",encoding='utf-8-sig')\n",
    "        regex = re.compile('[-:()\\\",.!?=`~/+&$%^*#]')\n",
    "        text = []\n",
    "        for aline in f.readlines():\n",
    "            aline = aline.lower()\n",
    "            aline = regex.sub('', aline).strip()\n",
    "            aline = re.sub(' +',' ',aline)\n",
    "            text.append(aline)\n",
    "        test_labels += [label]*len(text)\n",
    "        test_data += text\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-sussex",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 0, len: 9461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 1, len: 7315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 2, len: 9303\n",
      "{0, 1, 2}\n"
     ]
    }
   ],
   "source": [
    "folder_id = 1\r\n",
    "train_data, train_labels, test_data, test_labels = load_data(folder_id=folder_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4b4cf0-cf49-44e9-8832-f5b9803a7a9d",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-nlp in /opt/conda/lib/python3.8/site-packages (0.5.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from pytorch-nlp) (4.53.0)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from pytorch-nlp) (1.19.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-nlp\r\n",
    "from torchnlp.encoders.text import StaticTokenizerEncoder,pad_tensor, stack_and_pad_tensors\r\n",
    "\r\n",
    "encoder = StaticTokenizerEncoder(train_data, tokenize=lambda s: s.split())\r\n",
    "\r\n",
    "encoded_train_data = [encoder.encode(aline) for aline in train_data]\r\n",
    "encoded_train_data = [pad_tensor(x, length=512) for x in encoded_train_data]\r\n",
    "encoded_train_data = stack_and_pad_tensors(encoded_train_data)\r\n",
    "\r\n",
    "encoded_test_data = [encoder.encode(aline) for aline in test_data]\r\n",
    "encoded_test_data = [pad_tensor(x, length=512) for x in encoded_test_data]\r\n",
    "encoded_test_data = stack_and_pad_tensors(encoded_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-adobe",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fasttext in /opt/conda/lib/python3.8/site-packages (0.9.2)\r\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from fasttext) (50.3.1.post20201107)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from fasttext) (1.19.2)\r\n",
      "Requirement already satisfied: pybind11>=2.2 in /opt/conda/lib/python3.8/site-packages (from fasttext) (2.6.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install  fasttext\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6babe6ba-22a1-4d86-a85f-f2e52518971d",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "vocab_file = 'vocab_folder_{}.txt'.format(folder_id)\r\n",
    "with open(vocab_file,\"w\") as f:\r\n",
    "    for sentence in train_data:\r\n",
    "        l = sentence.split()\r\n",
    "        vocab+=l\r\n",
    "        if len(l)>0:\r\n",
    "            f.write(sentence+'\\n')\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061ff310-ab85-490f-aff9-2e8af665e90c",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "embedding_dim = 128\r\n",
    "\r\n",
    "cbow_model = fasttext.train_unsupervised(input = vocab_file, dim = embedding_dim, model = \"cbow\")\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79c4ab5-4975-4750-b476-acbfc92b3db8",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13204, 128)\n"
     ]
    }
   ],
   "source": [
    "weights_matrix = np.zeros((encoder.vocab_size, embedding_dim))\r\n",
    "word_ids = {}\r\n",
    "idx = 0\r\n",
    "\r\n",
    "for word in encoder.vocab:\r\n",
    "    weights_matrix[idx] = cbow_model.get_word_vector(word)\r\n",
    "    idx +=1\r\n",
    "print(weights_matrix.shape)\r\n",
    "\r\n",
    "def create_emb_layer(weights_matrix, non_trainable=False):\r\n",
    "    num_embeddings, embedding_dim = weights_matrix.shape\r\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\r\n",
    "    emb_layer.from_pretrained(torch.FloatTensor(weights_matrix))\r\n",
    "    if non_trainable:\r\n",
    "        emb_layer.weight.requires_grad = False\r\n",
    "\r\n",
    "    return emb_layer, num_embeddings, embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871443ba-169d-4313-a74a-02e83f5c3026",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "import torch.optim as optim\r\n",
    "from torch.autograd import Variable \r\n",
    "import time\r\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e565c48d-1a86-49b3-bc09-94870ab89023",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_labels = torch.Tensor(train_labels)\r\n",
    "dataset = torch.utils.data.TensorDataset(encoded_train_data[0], train_labels)\r\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=64,\r\n",
    "                                           pin_memory =True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227c4469-9309-4fa0-9c35-f14bfea1dff1",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "test_labels = torch.Tensor(test_labels)\r\n",
    "dataset = torch.utils.data.TensorDataset(encoded_test_data[0], test_labels)\r\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=64,\r\n",
    "                                           pin_memory =True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a76cc6e-de66-49c4-8c2a-bf4a98b45684",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_default_device():\r\n",
    "    \"\"\"Picking GPU if available or else CPU\"\"\"\r\n",
    "    if torch.cuda.is_available():\r\n",
    "        return torch.device('cuda')\r\n",
    "    else:\r\n",
    "        return torch.device('cpu')\r\n",
    "device = get_default_device()\r\n",
    "\r\n",
    "def to_device(data, device):\r\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\r\n",
    "    if isinstance(data, (list,tuple)):\r\n",
    "        return [to_device(x, device) for x in data]\r\n",
    "    return data.to(device, non_blocking=True)\r\n",
    "class DevicedataLoader():\r\n",
    "    def __init__(self,dl,device):\r\n",
    "        self.dl = dl\r\n",
    "        self.device = device\r\n",
    "    def __iter__(self):\r\n",
    "        for b in self.dl:\r\n",
    "            yield to_device(b, self.device)\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.dl)\r\n",
    "print(device)\r\n",
    "train_loader = DevicedataLoader(train_loader,device)\r\n",
    "test_loader = DevicedataLoader(test_loader,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-reminder",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, num_classes,  hidden_size, num_layers,weights_matrix):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "        # self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        # self.seq_length = seq_length\n",
    "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix,True)\n",
    "\n",
    "        # self.embedding = nn.Embedding( encoder.vocab_size, embedding_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size,\n",
    "                            num_layers = num_layers, bidirectional = False, batch_first = True)\n",
    "        self.fc_1 = nn.Linear(self.hidden_size, 64)\n",
    "        self.fc_2 = nn.Linear(64,num_classes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        # print(x)\n",
    "        # x = x.transpose(0,1)\n",
    "        len = torch.sum(x != 0, axis=1) -1\n",
    "        # print(len)\n",
    "        x = self.embedding(x)\n",
    "        # print(x)\n",
    "        # x, _ = self.lstm(x)\n",
    "        \n",
    "        x, (h_n, c_n)  = self.lstm(x)\n",
    "        # x = h_n[-1]\n",
    "        x = x[torch.arange(x.shape[0]),len,:]\n",
    "        # x = x[:,-1,:]\n",
    "        # x = x[-1]\n",
    "        x = self.relu(self.fc_1(x))\n",
    "        x = self.fc_2(x)\n",
    "        # print(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-ancient",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader, test_loader):\n",
    "    # model.load_state_dict(torch.load('folder_1_best.pth'))\n",
    "    highest_acc = 0.75\n",
    "    for epoch in range(1,n_epochs+1):\n",
    "        model.train()\n",
    "        loss_train = 0.0\n",
    "        for vectors, labels in train_loader:\n",
    "            vectors, labels = vectors.type(torch.LongTensor), labels.type(torch.LongTensor)\n",
    "            output = model(vectors.to(device))\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss = F.cross_entropy(output, labels.to(device))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train +=loss.item()\n",
    "        correct = 0\n",
    "        # break\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "\n",
    "                vec, label = data\n",
    "                vec, label = vec.type(torch.LongTensor), label.type(torch.LongTensor)\n",
    "                output = model(vec.to(device))\n",
    "                predicted = torch.argmax(output,1)\n",
    "                c = (predicted == label.to(device))\n",
    "                correct += c.sum()\n",
    "        if epoch == 1 or epoch % 1 == 0:\n",
    "            print('Epoch {}, Training loss {}, Val accuracy {}'.format(\n",
    "                epoch,\n",
    "                loss_train / len(train_loader),\n",
    "                correct / len(test_labels)))\n",
    "        if correct/len(test_labels) >= highest_acc:\n",
    "            highest_acc =  correct/len(test_labels)\n",
    "            file_name = \"folder_{}_pospadding_cbow.pth\".format(folder_id)\n",
    "            torch.save(model.state_dict(), file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-eight",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "input_size = embedding_dim\n",
    "hidden_size = embedding_dim\n",
    "num_layers = 5\n",
    "num_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-terminal",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "lstm_model = MyLSTM(num_classes,  hidden_size, num_layers, weights_matrix) #our lstm class \n",
    "lstm_model.to('cuda:0')\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-greenhouse",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 0.9355479627847672, Val accuracy 0.577646017074585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Training loss 0.7601457413505105, Val accuracy 0.5895054936408997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Training loss 0.6479029144872638, Val accuracy 0.6521593332290649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Training loss 0.5420408661970321, Val accuracy 0.6405236124992371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Training loss 0.43894272136921975, Val accuracy 0.6729693412780762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Training loss 0.3547736032884203, Val accuracy 0.6938912868499756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Training loss 0.27426201205554546, Val accuracy 0.6893041133880615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Training loss 0.2220098118882115, Val accuracy 0.6929962038993835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Training loss 0.1831983446700535, Val accuracy 0.6937794089317322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Training loss 0.15621511873277819, Val accuracy 0.6863951683044434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Training loss 0.13030051696570774, Val accuracy 0.7182815074920654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Training loss 0.120025334932257, Val accuracy 0.6935555934906006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Training loss 0.09664906490498278, Val accuracy 0.6966882944107056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Training loss 0.09389084155840215, Val accuracy 0.7105616331100464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Training loss 0.08841602266286774, Val accuracy 0.7140299677848816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Training loss 0.08123846209377471, Val accuracy 0.7100022435188293\n"
     ]
    }
   ],
   "source": [
    "training_loop(\n",
    "    n_epochs = 50,\n",
    "    optimizer = optimizer,\n",
    "    model = lstm_model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    "    test_loader = test_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573a0f22-4a53-4e98-b638-2dbac3a385ca",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "# pos-padding cbow\r\n",
    "- folder1 acc: 0.7414\r\n",
    "- folder2 acc: 0.7861\r\n",
    "- folder3 acc: 0.8224\r\n",
    "- folder4 acc: 0.8209\r\n",
    "- folder5 acc: 0.8194\r\n",
    "\r\n",
    "# pre-padding cbow\r\n",
    "- folder1 acc: 0.7355\r\n",
    "- folder2 acc: 0.7714\r\n",
    "- folder3 acc: 0.8085\r\n",
    "- folder4 acc: 0.7714\r\n",
    "- folder5 acc: 0.7195\r\n",
    "\r\n",
    "# folder 1:\r\n",
    "- pre_padding, CBOW: 0.7355\r\n",
    "- post_padding, CBOW: 0.7417\r\n",
    "- pre-padding, BPEmbedding: 0.7417\r\n",
    "- pos-padding, BPEmbedding: 0.75766\r\n",
    "# folder 2:\r\n",
    "- pre_padding, CBOW: 0.7714\r\n",
    "- post_padding, CBOW: 0.7861\r\n",
    "- pre-padding, BPEmbedding: 0.7659\r\n",
    "- post_padding, BPEmbedding: 0.7941\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d8b1ff-f712-4d07-8897-d06990bf40c0",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
